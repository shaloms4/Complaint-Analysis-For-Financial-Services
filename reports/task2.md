Chunking Strategy and Embedding Model Choice

Chunking Strategy:
The text chunking strategy uses LangChain’s RecursiveCharacterTextSplitter with a chunk_size of 256 tokens and a chunk_overlap of 20 tokens. This choice is informed by the Task 1 EDA, which showed that most complaint narratives range from 50 to 500 words (approximately 100-600 tokens). A chunk_size of 256 tokens ensures that chunks are small enough to capture specific issues (e.g., a single billing dispute) while retaining sufficient context for semantic understanding, aligning with the input limits of the all-MiniLM-L6-v2 model (max 512 tokens). The 20-token overlap preserves continuity across chunk boundaries, preventing loss of critical information in longer narratives. This configuration was tested against larger (512 tokens) and smaller (128 tokens) chunk sizes, with 256 tokens offering the best balance of retrieval accuracy and coherence for CrediTrust’s complaint analysis needs, as it avoids overly fragmented or overly broad embeddings.

Embedding Model Choice:
The sentence-transformers/all-MiniLM-L6-v2 model was selected for its efficiency and effectiveness in generating 384-dimensional embeddings, suitable for CrediTrust’s requirement to process thousands of complaints monthly. This lightweight model, trained on diverse text data, excels at capturing semantic similarity in short, complaint-focused texts, making it ideal for retrieving relevant narrative chunks in response to user queries. Compared to larger models like all-MPNet-base-v2, it offers faster computation and lower storage demands, critical for scaling with CrediTrust’s growing user base. FAISS was chosen as the vector store for its high-performance similarity search, capable of handling large datasets efficiently. Metadata, including Complaint ID, Product, chunk index, and original narrative, is stored alongside each embedding to enable traceability, supporting CrediTrust’s KPIs of empowering non-technical teams to quickly identify and analyze complaint trends.